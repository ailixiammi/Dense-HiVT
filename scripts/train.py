"""
Dense-HiVT æé€Ÿè®­ç»ƒå¼•æ“ (Train Loop)

æ ¸å¿ƒç‰¹æ€§:
1. è‡ªåŠ¨æ··åˆç²¾åº¦ (AMP) - å……åˆ†åˆ©ç”¨ RTX 4090 çš„ Tensor Core
2. AdamW ä¼˜åŒ–å™¨ + CosineAnnealingLR å­¦ä¹ ç‡è°ƒåº¦
3. æ¢¯åº¦è£å‰ª (Clip Grad Norm = 5.0) - é˜²æ­¢ Laplace NLL æ¢¯åº¦çˆ†ç‚¸
4. æœ€ä½³æ¨¡å‹å­˜æ¡£ (åŸºäº Val minFDE)
5. TensorBoard æ—¥å¿—è®°å½•
6. ä¼˜é›…çš„ç»ˆç«¯è¿›åº¦æ¡å’Œæ—¥å¿—è¾“å‡º

è¿è¡Œæ–¹å¼:
    python scripts/train.py
"""

import os
import sys
import time
import argparse
from pathlib import Path

import torch
import torch.nn as nn
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.model.dense_hivt import DenseHiVT
from src.model.loss import DenseHiVTLoss
from src.dataloader.dense_dataset import create_dataloaders
from src.trainer.metrics import compute_metrics


class TrainingEngine:
    """
    Dense-HiVT è®­ç»ƒå¼•æ“
    
    åŠŸèƒ½:
    - ç®¡ç†è®­ç»ƒå’ŒéªŒè¯å¾ªç¯
    - è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒ
    - å­¦ä¹ ç‡è°ƒåº¦
    - Checkpointing
    - TensorBoard æ—¥å¿—
    """
    
    def __init__(self, args):
        """
        åˆå§‹åŒ–è®­ç»ƒå¼•æ“
        
        Args:
            args: å‘½ä»¤è¡Œå‚æ•° (åŒ…å«è¶…å‚æ•°é…ç½®)
        """
        self.args = args
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path(args.output_dir)
        self.checkpoint_dir = self.output_dir / "checkpoints"
        self.log_dir = self.output_dir / "logs"
        
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ– TensorBoard
        self.writer = SummaryWriter(log_dir=str(self.log_dir))
        
        # åˆå§‹åŒ–æ¨¡å‹
        print()
        print("=" * 80)
        print("åˆå§‹åŒ– Dense-HiVT æ¨¡å‹".center(80))
        print("=" * 80)
        print()
        
        self.model = DenseHiVT(
            embed_dim=args.embed_dim,
            num_heads=args.num_heads,
            global_layers=args.num_global_interactor_layers,
            num_modes=args.num_modes,
            future_steps=args.future_steps,
            dropout=args.dropout
        ).to(self.device)
        
        print(f"æ¨¡å‹å·²åŠ è½½åˆ°è®¾å¤‡: {self.device}")
        print(f"æ€»å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters()) / 1e6:.2f}M")
        print()
        
        # åˆå§‹åŒ–æŸå¤±å‡½æ•°
        self.criterion = DenseHiVTLoss()
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=args.lr,
            weight_decay=args.weight_decay
        )
        
        # åˆå§‹åŒ–å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=args.epochs,
            eta_min=args.lr_min
        )
        
        # åˆå§‹åŒ– GradScaler (AMP)
        self.scaler = torch.cuda.amp.GradScaler(enabled=args.use_amp)
        
        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.best_val_fde = float('inf')
        self.global_step = 0
        
        print("=" * 80)
        print("âœ“ è®­ç»ƒå¼•æ“åˆå§‹åŒ–å®Œæˆ".center(80))
        print("=" * 80)
        print()
    
    def train_one_epoch(self, train_loader, epoch):
        """
        è®­ç»ƒä¸€ä¸ª Epoch
        
        Args:
            train_loader: è®­ç»ƒé›† DataLoader
            epoch: å½“å‰ Epoch ç¼–å·
        
        Returns:
            å¹³å‡è®­ç»ƒæŸå¤±
        """
        self.model.train()
        
        total_loss = 0.0
        total_reg_loss = 0.0
        total_cls_loss = 0.0
        
        # åˆ›å»ºè¿›åº¦æ¡
        pbar = tqdm(
            train_loader,
            desc=f"Epoch {epoch}/{self.args.epochs} [Train]",
            ncols=120,
            bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]'
        )
        
        for batch_idx, batch in enumerate(pbar):
            # ===================================================================
            # æ­¥éª¤ 1: æ•°æ®å‡†å¤‡ - æ¨é€åˆ° GPU
            # ===================================================================
            batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                    for k, v in batch.items()}
            
            # ===================================================================
            # æ­¥éª¤ 2: å‰å‘ä¼ æ’­ (ä½¿ç”¨ AMP)
            # ===================================================================
            self.optimizer.zero_grad()
            
            with torch.cuda.amp.autocast(enabled=self.args.use_amp):
                # æ¨¡å‹å‰å‘ä¼ æ’­
                outputs = self.model(batch)
                
                # è®¡ç®—æŸå¤±
                loss_dict = self.criterion(
                    pi=outputs['pi'],
                    loc=outputs['loc'],
                    y=batch['agent_future_positions'],
                    agent_current_pos=batch['agent_history_positions'][:, :, -1, :],  # [B, N, 2]
                    agent_current_heading=batch['agent_heading'],  # [B, N]
                    reg_mask=batch['agent_future_positions_mask'],
                    valid_mask=batch['agent_is_target']
                )
                
                loss = loss_dict['total_loss']
            
            # ===================================================================
            # æ­¥éª¤ 3: åå‘ä¼ æ’­ + æ¢¯åº¦è£å‰ª
            # ===================================================================
            self.scaler.scale(loss).backward()
            
            # æ¢¯åº¦è£å‰ª (é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸)
            self.scaler.unscale_(self.optimizer)
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(),
                max_norm=self.args.grad_clip_norm
            )
            
            # ä¼˜åŒ–å™¨æ›´æ–°
            self.scaler.step(self.optimizer)
            self.scaler.update()
            
            # ===================================================================
            # æ­¥éª¤ 4: è®°å½•æŸå¤±
            # ===================================================================
            total_loss += loss.item()
            total_reg_loss += loss_dict['reg_loss'].item()
            total_cls_loss += loss_dict['cls_loss'].item()
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'loss': f"{loss.item():.4f}",
                'reg': f"{loss_dict['reg_loss'].item():.4f}",
                'cls': f"{loss_dict['cls_loss'].item():.4f}",
                'lr': f"{self.optimizer.param_groups[0]['lr']:.6f}"
            })
            
            # TensorBoard è®°å½• (æ¯ 100 æ­¥)
            if self.global_step % 100 == 0:
                self.writer.add_scalar('Train/Loss', loss.item(), self.global_step)
                self.writer.add_scalar('Train/RegLoss', loss_dict['reg_loss'].item(), self.global_step)
                self.writer.add_scalar('Train/ClsLoss', loss_dict['cls_loss'].item(), self.global_step)
                self.writer.add_scalar('Train/LR', self.optimizer.param_groups[0]['lr'], self.global_step)
            
            self.global_step += 1
        
        # è®¡ç®—å¹³å‡æŸå¤±
        avg_loss = total_loss / len(train_loader)
        avg_reg_loss = total_reg_loss / len(train_loader)
        avg_cls_loss = total_cls_loss / len(train_loader)
        
        return {
            'loss': avg_loss,
            'reg_loss': avg_reg_loss,
            'cls_loss': avg_cls_loss
        }
    
    @torch.no_grad()
    def validate(self, val_loader, epoch):
        """
        éªŒè¯æ¨¡å‹æ€§èƒ½
        
        Args:
            val_loader: éªŒè¯é›† DataLoader
            epoch: å½“å‰ Epoch ç¼–å·
        
        Returns:
            éªŒè¯æŒ‡æ ‡å­—å…¸ (minADE, minFDE, MR)
        """
        self.model.eval()
        
        # ç´¯ç§¯è¯„æµ‹æŒ‡æ ‡
        total_ade = 0.0
        total_fde = 0.0
        total_mr = 0.0
        num_batches = 0
        
        # åˆ›å»ºè¿›åº¦æ¡
        pbar = tqdm(
            val_loader,
            desc=f"Epoch {epoch}/{self.args.epochs} [Val]  ",
            ncols=120,
            bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]'
        )
        
        for batch in pbar:
            # ===================================================================
            # æ­¥éª¤ 1: æ•°æ®å‡†å¤‡
            # ===================================================================
            batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                    for k, v in batch.items()}
            
            # ===================================================================
            # æ­¥éª¤ 2: å‰å‘ä¼ æ’­ (æ— æ¢¯åº¦)
            # ===================================================================
            with torch.cuda.amp.autocast(enabled=self.args.use_amp):
                outputs = self.model(batch)
            
            # ===================================================================
            # æ­¥éª¤ 3: æå–é¢„æµ‹è½¨è¿¹ä½ç½® (å¿½ç•¥å°ºåº¦å‚æ•°)
            # ===================================================================
            # outputs['loc']: [B, N, K, F, 4] (æœ€åä¸€ç»´: [Î¼_x, Î¼_y, b_x, b_y])
            # æˆ‘ä»¬åªéœ€è¦ä½ç½®é¢„æµ‹: [B, N, K, F, 2]
            pred_trajs = outputs['loc'][..., :2]  # [B, N, K, F, 2]
            
            # ===================================================================
            # æ­¥éª¤ 4: è®¡ç®—è¯„æµ‹æŒ‡æ ‡
            # ===================================================================
            metrics = compute_metrics(
                pred_trajs=pred_trajs,
                gt_trajs=batch['agent_future_positions'],
                gt_masks=batch['agent_future_positions_mask'],
                target_masks=batch['agent_is_target'],
                miss_threshold=2.0
            )
            
            # ç´¯åŠ æŒ‡æ ‡ (å¿½ç•¥ NaN)
            if not torch.isnan(torch.tensor(metrics['minADE'])):
                total_ade += metrics['minADE']
                total_fde += metrics['minFDE']
                total_mr += metrics['MR']
                num_batches += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'ADE': f"{metrics['minADE']:.4f}",
                'FDE': f"{metrics['minFDE']:.4f}",
                'MR': f"{metrics['MR']:.2%}"
            })
        
        # ===================================================================
        # æ­¥éª¤ 5: è®¡ç®—å¹³å‡æŒ‡æ ‡
        # ===================================================================
        avg_ade = total_ade / num_batches if num_batches > 0 else float('nan')
        avg_fde = total_fde / num_batches if num_batches > 0 else float('nan')
        avg_mr = total_mr / num_batches if num_batches > 0 else float('nan')
        
        return {
            'minADE': avg_ade,
            'minFDE': avg_fde,
            'MR': avg_mr
        }
    
    def save_checkpoint(self, epoch, val_metrics, is_best=False):
        """
        ä¿å­˜æ¨¡å‹ Checkpoint
        
        Args:
            epoch: å½“å‰ Epoch
            val_metrics: éªŒè¯æŒ‡æ ‡
            is_best: æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
        """
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'scaler_state_dict': self.scaler.state_dict(),
            'best_val_fde': self.best_val_fde,
            'val_metrics': val_metrics,
            'args': vars(self.args)
        }
        
        # ä¿å­˜æœ€æ–°çš„ Checkpoint
        latest_path = self.checkpoint_dir / "latest.pth"
        torch.save(checkpoint, latest_path)
        
        # å¦‚æœæ˜¯æœ€ä½³æ¨¡å‹ï¼Œé¢å¤–ä¿å­˜ä¸€ä»½
        if is_best:
            best_path = self.checkpoint_dir / "best_dense_hivt.pth"
            torch.save(checkpoint, best_path)
            print(f"\nâœ“ æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {best_path}")
            print(f"  - minFDE: {val_metrics['minFDE']:.4f} ç±³")
    
    def train(self, train_loader, val_loader):
        """
        ä¸»è®­ç»ƒå¾ªç¯
        
        Args:
            train_loader: è®­ç»ƒé›† DataLoader
            val_loader: éªŒè¯é›† DataLoader
        """
        print()
        print("=" * 80)
        print("å¼€å§‹è®­ç»ƒ".center(80))
        print("=" * 80)
        print()
        print(f"æ€» Epochs: {self.args.epochs}")
        print(f"è®­ç»ƒé›†å¤§å°: {len(train_loader.dataset)} æ ·æœ¬")
        print(f"éªŒè¯é›†å¤§å°: {len(val_loader.dataset)} æ ·æœ¬")
        print(f"Base LR: {self.args.lr}")
        print(f"Weight Decay: {self.args.weight_decay}")
        print(f"Gradient Clip Norm: {self.args.grad_clip_norm}")
        print(f"AMP å¯ç”¨: {self.args.use_amp}")
        print()
        print("=" * 80)
        print()
        
        for epoch in range(1, self.args.epochs + 1):
            self.current_epoch = epoch
            
            # ===================================================================
            # è®­ç»ƒä¸€ä¸ª Epoch
            # ===================================================================
            train_metrics = self.train_one_epoch(train_loader, epoch)
            
            # ===================================================================
            # éªŒè¯æ¨¡å‹
            # ===================================================================
            val_metrics = self.validate(val_loader, epoch)
            
            # ===================================================================
            # å­¦ä¹ ç‡è°ƒåº¦
            # ===================================================================
            self.scheduler.step()
            
            # ===================================================================
            # TensorBoard è®°å½• Epoch çº§åˆ«æŒ‡æ ‡
            # ===================================================================
            self.writer.add_scalar('Epoch/Train_Loss', train_metrics['loss'], epoch)
            self.writer.add_scalar('Epoch/Val_minADE', val_metrics['minADE'], epoch)
            self.writer.add_scalar('Epoch/Val_minFDE', val_metrics['minFDE'], epoch)
            self.writer.add_scalar('Epoch/Val_MR', val_metrics['MR'], epoch)
            self.writer.add_scalar('Epoch/LR', self.optimizer.param_groups[0]['lr'], epoch)
            
            # ===================================================================
            # æ‰“å° Epoch æ€»ç»“
            # ===================================================================
            print()
            print("=" * 80)
            print(f"Epoch {epoch}/{self.args.epochs} æ€»ç»“".center(80))
            print("=" * 80)
            print()
            print(f"[è®­ç»ƒ]")
            print(f"  - Total Loss: {train_metrics['loss']:.4f}")
            print(f"  - Reg Loss:   {train_metrics['reg_loss']:.4f}")
            print(f"  - Cls Loss:   {train_metrics['cls_loss']:.4f}")
            print()
            print(f"[éªŒè¯]")
            print(f"  - minADE: {val_metrics['minADE']:.4f} ç±³")
            print(f"  - minFDE: {val_metrics['minFDE']:.4f} ç±³")
            print(f"  - MR:     {val_metrics['MR']:.2%}")
            print()
            print(f"[ä¼˜åŒ–å™¨]")
            print(f"  - Learning Rate: {self.optimizer.param_groups[0]['lr']:.6f}")
            print()
            
            # ===================================================================
            # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
            # ===================================================================
            is_best = False
            if val_metrics['minFDE'] < self.best_val_fde:
                self.best_val_fde = val_metrics['minFDE']
                is_best = True
                print(f"ğŸ‰ æ–°çš„æœ€ä½³ minFDE: {self.best_val_fde:.4f} ç±³")
                print()
            
            # ===================================================================
            # ä¿å­˜ Checkpoint
            # ===================================================================
            self.save_checkpoint(epoch, val_metrics, is_best=is_best)
            
            print("=" * 80)
            print()
        
        # ===================================================================
        # è®­ç»ƒç»“æŸ
        # ===================================================================
        print()
        print("=" * 80)
        print("è®­ç»ƒå®Œæˆï¼".center(80))
        print("=" * 80)
        print()
        print(f"æœ€ä½³éªŒè¯ minFDE: {self.best_val_fde:.4f} ç±³")
        print(f"Checkpoints ä¿å­˜ä½ç½®: {self.checkpoint_dir}")
        print(f"TensorBoard æ—¥å¿—: {self.log_dir}")
        print()
        print("=" * 80)
        
        self.writer.close()


def parse_args():
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°
    
    Returns:
        è§£æåçš„å‚æ•°å¯¹è±¡
    """
    parser = argparse.ArgumentParser(description="Dense-HiVT è®­ç»ƒè„šæœ¬")
    
    # =========================================================================
    # æ•°æ®ç›¸å…³
    # =========================================================================
    parser.add_argument(
        '--train_dir',
        type=str,
        default='/root/devdata/Dense-HiVT/data/processed/train',
        help='è®­ç»ƒé›†ç›®å½•è·¯å¾„'
    )
    parser.add_argument(
        '--val_dir',
        type=str,
        default='/root/devdata/Dense-HiVT/data/processed/val',
        help='éªŒè¯é›†ç›®å½•è·¯å¾„'
    )
    parser.add_argument(
        '--output_dir',
        type=str,
        default='outputs',
        help='è¾“å‡ºç›®å½•ï¼ˆCheckpoints + Logsï¼‰'
    )
    
    # =========================================================================
    # æ¨¡å‹è¶…å‚æ•°
    # =========================================================================
    parser.add_argument('--embed_dim', type=int, default=128, help='åµŒå…¥ç»´åº¦')
    parser.add_argument('--num_heads', type=int, default=8, help='Multi-Head Attention å¤´æ•°')
    parser.add_argument('--num_local_encoder_layers', type=int, default=4, help='Local Encoder å±‚æ•°')
    parser.add_argument('--num_global_interactor_layers', type=int, default=3, help='Global Interactor å±‚æ•°')
    parser.add_argument('--num_decoder_layers', type=int, default=4, help='Decoder å±‚æ•°')
    parser.add_argument('--dropout', type=float, default=0.1, help='Dropout æ¦‚ç‡')
    parser.add_argument('--num_modes', type=int, default=6, help='é¢„æµ‹æ¨¡æ€æ•°')
    parser.add_argument('--future_steps', type=int, default=30, help='æœªæ¥æ—¶é—´æ­¥æ•°')
    
    # =========================================================================
    # è®­ç»ƒè¶…å‚æ•°
    # =========================================================================
    parser.add_argument('--epochs', type=int, default=64, help='æ€»è®­ç»ƒ Epochs')
    parser.add_argument('--batch_size', type=int, default=64, help='è®­ç»ƒ Batch Size')
    parser.add_argument('--lr', type=float, default=1e-3, help='Base Learning Rate')
    parser.add_argument('--lr_min', type=float, default=1e-6, help='æœ€å°å­¦ä¹ ç‡ï¼ˆCosineAnnealingï¼‰')
    parser.add_argument('--weight_decay', type=float, default=1e-4, help='Weight Decay (AdamW)')
    parser.add_argument('--grad_clip_norm', type=float, default=5.0, help='æ¢¯åº¦è£å‰ªé˜ˆå€¼')
    parser.add_argument('--use_amp', action='store_true', default=True, help='ä½¿ç”¨ AMPï¼ˆè‡ªåŠ¨æ··åˆç²¾åº¦ï¼‰')
    
    # =========================================================================
    # DataLoader é…ç½®
    # =========================================================================
    parser.add_argument('--num_workers', type=int, default=8, help='DataLoader è¿›ç¨‹æ•°')
    parser.add_argument('--pin_memory', action='store_true', default=True, help='ä½¿ç”¨ Pin Memory')
    parser.add_argument('--prefetch_factor', type=int, default=2, help='é¢„å–æ‰¹æ¬¡æ•°')
    
    return parser.parse_args()


def main():
    """
    ä¸»å‡½æ•°å…¥å£
    """
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_args()
    
    # æ‰“å°å¯åŠ¨ä¿¡æ¯
    print()
    print("=" * 80)
    print("Dense-HiVT æé€Ÿè®­ç»ƒå¼•æ“".center(80))
    print("=" * 80)
    print()
    
    # æ£€æŸ¥ CUDA å¯ç”¨æ€§
    if not torch.cuda.is_available():
        print("âš ï¸  è­¦å‘Š: CUDA ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ CPU è®­ç»ƒï¼ˆæ€§èƒ½ä¼šå¤§å¹…ä¸‹é™ï¼‰")
        print()
    else:
        print(f"âœ“ GPU è®¾å¤‡: {torch.cuda.get_device_name(0)}")
        print(f"âœ“ CUDA ç‰ˆæœ¬: {torch.version.cuda}")
        print()
    
    # =========================================================================
    # åˆ›å»º DataLoader
    # =========================================================================
    train_loader, val_loader = create_dataloaders(
        train_dir=args.train_dir,
        val_dir=args.val_dir,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        pin_memory=args.pin_memory,
        prefetch_factor=args.prefetch_factor
    )
    
    # =========================================================================
    # åˆå§‹åŒ–è®­ç»ƒå¼•æ“
    # =========================================================================
    engine = TrainingEngine(args)
    
    # =========================================================================
    # å¼€å§‹è®­ç»ƒ
    # =========================================================================
    try:
        engine.train(train_loader, val_loader)
    except KeyboardInterrupt:
        print()
        print("=" * 80)
        print("è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­".center(80))
        print("=" * 80)
        print()
        engine.writer.close()
    except Exception as e:
        print()
        print("=" * 80)
        print("è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯".center(80))
        print("=" * 80)
        print()
        print(f"é”™è¯¯ä¿¡æ¯: {e}")
        import traceback
        traceback.print_exc()
        engine.writer.close()


if __name__ == "__main__":
    main()